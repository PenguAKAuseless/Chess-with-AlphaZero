{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b822c53d",
   "metadata": {},
   "source": [
    "In this notebook we are going through steps to setup a simple chess board AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67b57376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chess in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (1.11.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3996ebd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>r n b q k b n r\n",
       "p p p p p p p p\n",
       ". . . . . . . .\n",
       ". . . . . . . .\n",
       ". . . . . . . .\n",
       ". . . . . . . .\n",
       "P P P P P P P P\n",
       "R N B Q K B N R</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\" /><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(60, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(150, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(285, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(195, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(60, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(105, 15)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(150, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(195, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(240, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(285, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(330, 15)\" /></svg>"
      ],
      "text/plain": [
       "Board('rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chess\n",
    "\n",
    "board = chess.Board()\n",
    "# Simple chess board for functioning my AI\n",
    "board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a0a95",
   "metadata": {},
   "source": [
    "We can use this for our agents playground. Time to setup the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f790ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stockfish in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (3.28.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stockfish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1eb7bf",
   "metadata": {},
   "source": [
    "Now we have the opponent to test ground truth.\n",
    "For making an AlphaZero model, we will need it in the very late stage - when the model is almost done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bb1da75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (2.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71c3c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We build the ChessState for AlphaZero to evaluate first\n",
    "class ChessState:\n",
    "    def __init__(self, board: chess.Board):\n",
    "        self.board = board\n",
    "\n",
    "    def clone(self):\n",
    "        return ChessState(self.board.copy())\n",
    "\n",
    "    def apply_move(self, move: chess.Move):\n",
    "        self.board.push(move)\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.board.is_game_over()\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.board.legal_moves)\n",
    "\n",
    "    def result(self):\n",
    "        return self.board.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee0076",
   "metadata": {},
   "source": [
    "Next we focus on Deep Neural Network\n",
    "This network focus on learning the probability of the next move -> the policy of the game, the value for the current player -> the game ultimate winner following this move\n",
    "\n",
    "Input (C, 8, 8) (game state)\n",
    "↓\n",
    "Conv Layer (Conv 3x3 x 256, BN, ReLU)\n",
    "↓\n",
    "Residual Layer × N (Conv 3x3 x 256, BN, ReLU, Conv 3x3 x 256, BN, Skip Connection, ReLU) # This skip is important to help model learn, avoid desc grad\n",
    "↓\n",
    "↓-------------------↓\n",
    "|                   |\n",
    "Policy Head         Value Head\n",
    "Conv 1×1 x 2        Conv 1×1\n",
    "BN, ReLU            BN, ReLU\n",
    "FC (to 4672)        FC (hidden, 256) → ReLU → FC (1) → tanh\n",
    "\n",
    "Now there might be some confoosion (kinda) around 4672.\n",
    "4672 = 8x8x73\n",
    "= 8x8 x (56 directional offsets + 8 knight moves + 9 promotions)\n",
    "Now you may be more CONFOOSION, extra promotion just dropped out of somewhere\n",
    "This is for strong model to learn underpromotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0636810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (80.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: h5py in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from h5py) (2.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5f09432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just jump directly into making layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "# Input Convolutional Layer\n",
    "class InputConvolutionalLayer(nn.Module):\n",
    "    def __init__(self, in_channels=103, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def load_from_hdf5(self, h5_group):\n",
    "        self.conv.weight.data = torch.tensor(h5_group['conv_weight'][:])\n",
    "        self.conv.bias.data = torch.tensor(h5_group['conv_bias'][:])\n",
    "        self.bn.weight.data = torch.tensor(h5_group['bn_weight'][:])\n",
    "        self.bn.bias.data = torch.tensor(h5_group['bn_bias'][:])\n",
    "        self.bn.running_mean.data = torch.tensor(h5_group['bn_running_mean'][:])\n",
    "        self.bn.running_var.data = torch.tensor(h5_group['bn_running_var'][:])\n",
    "    \n",
    "# Residual Layer\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, channels=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2= nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2= nn.BatchNorm2d(num_features=channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        residual = self.conv1(residual)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.relu1(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "        residual = residual + x\n",
    "        residual = self.relu2(residual)\n",
    "        return residual\n",
    "    \n",
    "    def load_from_hdf5(self, h5_group):\n",
    "        self.conv1.weight.data = torch.tensor(h5_group['conv1_weight'][:])\n",
    "        self.conv1.bias.data = torch.tensor(h5_group['conv1_bias'][:])\n",
    "        self.bn1.weight.data = torch.tensor(h5_group['bn1_weight'][:])\n",
    "        self.bn1.bias.data = torch.tensor(h5_group['bn1_bias'][:])\n",
    "        self.bn1.running_mean.data = torch.tensor(h5_group['bn1_running_mean'][:])\n",
    "        self.bn1.running_var.data = torch.tensor(h5_group['bn1_running_var'][:])\n",
    "        self.conv2.weight.data = torch.tensor(h5_group['conv2_weight'][:])\n",
    "        self.conv2.bias.data = torch.tensor(h5_group['conv2_bias'][:])\n",
    "        self.bn2.weight.data = torch.tensor(h5_group['bn2_weight'][:])\n",
    "        self.bn2.bias.data = torch.tensor(h5_group['bn2_bias'][:])\n",
    "        self.bn2.running_mean.data = torch.tensor(h5_group['bn2_running_mean'][:])\n",
    "        self.bn2.running_var.data = torch.tensor(h5_group['bn2_running_var'][:])\n",
    "\n",
    "# Value Head\n",
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size):\n",
    "        super(ValueHead, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(1)\n",
    "        self.fcl1 = nn.Linear(64, hidden_size)  # 8x8 board\n",
    "        self.fcl2 = nn.Linear(hidden_size, 1)   # Fix: Output a single value\n",
    "        self.activation = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.bn(self.conv(x)))\n",
    "        x = x.view(-1, 64)  # Flatten\n",
    "        x = self.activation(self.fcl1(x))\n",
    "        x = self.tanh(self.fcl2(x))  # Output shape: [batch_size, 1]\n",
    "        return x\n",
    "    \n",
    "    def load_from_hdf5(self, h5_group):\n",
    "        self.conv.weight.data = torch.tensor(h5_group['conv_weight'][:])\n",
    "        self.conv.bias.data = torch.tensor(h5_group['conv_bias'][:])\n",
    "        self.bn.weight.data = torch.tensor(h5_group['bn_weight'][:])\n",
    "        self.bn.bias.data = torch.tensor(h5_group['bn_bias'][:])\n",
    "        self.bn.running_mean.data = torch.tensor(h5_group['bn_running_mean'][:])\n",
    "        self.bn.running_var.data = torch.tensor(h5_group['bn_running_var'][:])\n",
    "        self.fcl1.weight.data = torch.tensor(h5_group['fcl1_weight'][:])\n",
    "        self.fcl1.bias.data = torch.tensor(h5_group['fcl1_bias'][:])\n",
    "        self.fcl2.weight.data = torch.tensor(h5_group['fcl2_weight'][:])\n",
    "        self.fcl2.bias.data = torch.tensor(h5_group['fcl2_bias'][:])\n",
    "    \n",
    "# Policy Head\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0)\n",
    "        self.bn = nn.BatchNorm2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fcl = nn.Linear(in_features = 8 * 8 * 2, out_features = 8 * 8 * (56 + 8 + 9))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fcl(x)\n",
    "        return x\n",
    "    \n",
    "    def load_from_hdf5(self, h5_group):\n",
    "        self.conv.weight.data = torch.tensor(h5_group['conv_weight'][:])\n",
    "        self.conv.bias.data = torch.tensor(h5_group['conv_bias'][:])\n",
    "        self.bn.weight.data = torch.tensor(h5_group['bn_weight'][:])\n",
    "        self.bn.bias.data = torch.tensor(h5_group['bn_bias'][:])\n",
    "        self.bn.running_mean.data = torch.tensor(h5_group['bn_running_mean'][:])\n",
    "        self.bn.running_var.data = torch.tensor(h5_group['bn_running_var'][:])\n",
    "        self.fcl.weight.data = torch.tensor(h5_group['fcl_weight'][:])\n",
    "        self.fcl.bias.data = torch.tensor(h5_group['fcl_bias'][:])\n",
    "    \n",
    "# Combination is power\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=103, out_channels=256, num_residual=20, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.conv = InputConvolutionalLayer(in_channels=in_channels, out_channels=out_channels)\n",
    "        self.residual_list = nn.Sequential(*[ResidualLayer(channels=out_channels) for _ in range(20)])\n",
    "        self.value_head = ValueHead(in_channels=out_channels, hidden_size=hidden_size)\n",
    "        self.policy_head = PolicyHead(channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.residual_list(x)\n",
    "        # Clone x for both head\n",
    "        value_x = x.clone()\n",
    "        policy_x = x\n",
    "        return self.value_head(value_x), self.policy_head(policy_x)\n",
    "    \n",
    "    def save_to_hdf5(self, filename):\n",
    "        with h5py.File(filename, 'w') as f:\n",
    "            # Save InputConvolutionalLayer\n",
    "            conv_group = f.create_group('conv')\n",
    "            conv_group.create_dataset('conv_weight', data=self.conv.conv.weight.data.cpu().numpy())\n",
    "            conv_group.create_dataset('conv_bias', data=self.conv.conv.bias.data.cpu().numpy())\n",
    "            conv_group.create_dataset('bn_weight', data=self.conv.bn.weight.data.cpu().numpy())\n",
    "            conv_group.create_dataset('bn_bias', data=self.conv.bn.bias.data.cpu().numpy())\n",
    "            conv_group.create_dataset('bn_running_mean', data=self.conv.bn.running_mean.data.cpu().numpy())\n",
    "            conv_group.create_dataset('bn_running_var', data=self.conv.bn.running_var.data.cpu().numpy())\n",
    "\n",
    "            # Save Residual Layers\n",
    "            for i, residual in enumerate(self.residual_list):\n",
    "                res_group = f.create_group(f'residual_{i}')\n",
    "                res_group.create_dataset('conv1_weight', data=residual.conv1.weight.data.cpu().numpy())\n",
    "                res_group.create_dataset('conv1_bias', data=residual.conv1.bias.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn1_weight', data=residual.bn1.weight.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn1_bias', data=residual.bn1.bias.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn1_running_mean', data=residual.bn1.running_mean.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn1_running_var', data=residual.bn1.running_var.data.cpu().numpy())\n",
    "                res_group.create_dataset('conv2_weight', data=residual.conv2.weight.data.cpu().numpy())\n",
    "                res_group.create_dataset('conv2_bias', data=residual.conv2.bias.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn2_weight', data=residual.bn2.weight.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn2_bias', data=residual.bn2.bias.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn2_running_mean', data=residual.bn2.running_mean.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn2_running_var', data=residual.bn2.running_var.data.cpu().numpy())\n",
    "\n",
    "            # Save Value Head\n",
    "            value_group = f.create_group('value_head')\n",
    "            value_group.create_dataset('conv_weight', data=self.value_head.conv.weight.data.cpu().numpy())\n",
    "            value_group.create_dataset('conv_bias', data=self.value_head.conv.bias.data.cpu().numpy())\n",
    "            value_group.create_dataset('bn_weight', data=self.value_head.bn.weight.data.cpu().numpy())\n",
    "            value_group.create_dataset('bn_bias', data=self.value_head.bn.bias.data.cpu().numpy())\n",
    "            value_group.create_dataset('bn_running_mean', data=self.value_head.bn.running_mean.data.cpu().numpy())\n",
    "            value_group.create_dataset('bn_running_var', data=self.value_head.bn.running_var.data.cpu().numpy())\n",
    "            value_group.create_dataset('fcl1_weight', data=self.value_head.fcl1.weight.data.cpu().numpy())\n",
    "            value_group.create_dataset('fcl1_bias', data=self.value_head.fcl1.bias.data.cpu().numpy())\n",
    "            value_group.create_dataset('fcl2_weight', data=self.value_head.fcl2.weight.data.cpu().numpy())\n",
    "            value_group.create_dataset('fcl2_bias', data=self.value_head.fcl2.bias.data.cpu().numpy())\n",
    "\n",
    "            # Save Policy Head\n",
    "            policy_group = f.create_group('policy_head')\n",
    "            policy_group.create_dataset('conv_weight', data=self.policy_head.conv.weight.data.cpu().numpy())\n",
    "            policy_group.create_dataset('conv_bias', data=self.policy_head.conv.bias.data.cpu().numpy())\n",
    "            policy_group.create_dataset('bn_weight', data=self.policy_head.bn.weight.data.cpu().numpy())\n",
    "            policy_group.create_dataset('bn_bias', data=self.policy_head.bn.bias.data.cpu().numpy())\n",
    "            policy_group.create_dataset('bn_running_mean', data=self.policy_head.bn.running_mean.data.cpu().numpy())\n",
    "            policy_group.create_dataset('bn_running_var', data=self.policy_head.bn.running_var.data.cpu().numpy())\n",
    "            policy_group.create_dataset('fcl_weight', data=self.policy_head.fcl.weight.data.cpu().numpy())\n",
    "            policy_group.create_dataset('fcl_bias', data=self.policy_head.fcl.bias.data.cpu().numpy())\n",
    "\n",
    "    def load_from_hdf5(self, filename):\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            self.conv.load_from_hdf5(f['conv'])\n",
    "            for i, residual in enumerate(self.residual_list):\n",
    "                residual.load_from_hdf5(f[f'residual_{i}'])\n",
    "            self.value_head.load_from_hdf5(f['value_head'])\n",
    "            self.policy_head.load_from_hdf5(f['policy_head'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88655da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost perfect, but wait: The input is not ready\n",
    "# Function to convert board list to tensor\n",
    "def boards_to_tensor(history):\n",
    "    tensor = np.zeros((103, 8, 8), dtype=np.float32)\n",
    "    piece_planes = {\n",
    "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11,\n",
    "    }\n",
    "    # Piece planes (up to 8 timesteps)\n",
    "    for t, chess_state in enumerate(history[-8:]):  # Take last 8 states\n",
    "        board = chess_state.board\n",
    "        offset = t * 12\n",
    "        for square, piece in board.piece_map().items():\n",
    "            row = 7 - chess.square_rank(square)\n",
    "            col = chess.square_file(square)\n",
    "            plane = offset + piece_planes[piece.symbol()]\n",
    "            tensor[plane, row, col] = 1\n",
    "    latest_board = history[-1].board\n",
    "    tensor[96, :, :] = int(latest_board.turn)\n",
    "    tensor[97, :, :] = int(latest_board.has_kingside_castling_rights(chess.WHITE))\n",
    "    tensor[98, :, :] = int(latest_board.has_queenside_castling_rights(chess.WHITE))\n",
    "    tensor[99, :, :] = int(latest_board.has_kingside_castling_rights(chess.BLACK))\n",
    "    tensor[100, :, :] = int(latest_board.has_queenside_castling_rights(chess.BLACK))\n",
    "    tensor[101, :, :] = latest_board.halfmove_clock / 100.0\n",
    "    tensor[102, :, :] = latest_board.fullmove_number / 100.0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4604a",
   "metadata": {},
   "source": [
    "So far so good.\n",
    "We are gonna move to Monte Carlo Tree Search this time\n",
    "For overview, this is our tree setup: From a node state s, we will have action node a contains:\n",
    "- Visit count N(s, a)\n",
    "- Total value W(s, a)\n",
    "- Mean action value Q(s, a) = W(s, a) / N(s, a)\n",
    "- Prio probability P(s, a)\n",
    "Traversal will use PUCT formula: a_t = argmax(Q(s, a) + U(s, a))\n",
    "with U(s, a) = c_puct * P(s, a) * sqrt(SUM N(s, b)) / (1 + N(s, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abd64a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloTreeNode:\n",
    "    def __init__(self, state : ChessState, parent=None, prior=0.0):\n",
    "        self.state = state              # ChessState\n",
    "        self.parent = parent            # MonteCarloTreeNode\n",
    "        self.prior = prior              # from policy network\n",
    "        self.children = {}              # move (chess.Move) -> MonteCarloTreeNode\n",
    "        self.visits = 0\n",
    "        self.value_sum = 0.0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        return self.value_sum / self.visits if self.visits > 0 else 0\n",
    "\n",
    "    def expand(self, move_priors):\n",
    "        for move, prior in move_priors.items():\n",
    "            next_state = self.state.clone()\n",
    "            next_state.apply_move(move)\n",
    "            self.children[move] = MonteCarloTreeNode(next_state, parent=self, prior=prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1e7a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def evaluate(self, history):\n",
    "        tensor = boards_to_tensor(history)\n",
    "        input_tensor = torch.tensor(tensor).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            value, policy_logits= self.model(input_tensor)\n",
    "\n",
    "        policy = F.softmax(policy_logits, dim=1).squeeze(0).cpu().numpy()\n",
    "        value = value.item()\n",
    "\n",
    "        # Map policy logits to legal moves\n",
    "        legal_moves = history[-1].legal_moves()\n",
    "        move_priors = {}\n",
    "        for move in legal_moves:\n",
    "            idx = self.move_to_index(move)  # needs matching encoding\n",
    "            move_priors[move] = policy[idx]\n",
    "\n",
    "        return move_priors, value\n",
    "\n",
    "    def move_to_index(self, move: chess.Move) -> int:\n",
    "        from_square = move.from_square\n",
    "        to_square = move.to_square\n",
    "        from_row = 7 - chess.square_rank(from_square)\n",
    "        from_col = chess.square_file(from_square)\n",
    "        to_row = 7 - chess.square_rank(to_square)\n",
    "        to_col = chess.square_file(to_square)\n",
    "\n",
    "        delta_row = to_row - from_row\n",
    "        delta_col = to_col - from_col\n",
    "\n",
    "        # === 1. Directional moves: 8 directions × 7 steps = 56 planes ===\n",
    "        if move.promotion is None:\n",
    "            directions = [\n",
    "                (0, 1),   # → right\n",
    "                (0, -1),  # ← left\n",
    "                (1, 0),   # ↑ up\n",
    "                (-1, 0),  # ↓ down\n",
    "                (1, 1),   # ↗\n",
    "                (1, -1),  # ↖\n",
    "                (-1, 1),  # ↘\n",
    "                (-1, -1)  # ↙\n",
    "            ]\n",
    "\n",
    "            for dir_idx, (dr, dc) in enumerate(directions):\n",
    "                for step in range(1, 8):  # up to 7 squares\n",
    "                    if delta_row == dr * step and delta_col == dc * step:\n",
    "                        plane = dir_idx * 7 + (step - 1)\n",
    "                        return (from_row * 8 + from_col) * 73 + plane\n",
    "\n",
    "        # === 2. Knight moves: 8 planes ===\n",
    "        knight_deltas = [\n",
    "            (2, 1), (2, -1), (-2, 1), (-2, -1),\n",
    "            (1, 2), (1, -2), (-1, 2), (-1, -2)\n",
    "        ]\n",
    "        for i, (dr, dc) in enumerate(knight_deltas):\n",
    "            if delta_row == dr and delta_col == dc:\n",
    "                return (from_row * 8 + from_col) * 73 + 56 + i\n",
    "\n",
    "        # === 3. Promotion moves: 3 directions × 4 pieces (knight, bishop, rook, queen) = 12 planes (64–75) ===\n",
    "        if move.promotion in {chess.KNIGHT, chess.BISHOP, chess.ROOK, chess.QUEEN}:\n",
    "            # Check if it's a valid promotion move (pawns on second-to-last rank)\n",
    "            # For white: from_row is 1 (second rank from top)\n",
    "            # For black: from_row is 6 (second rank from bottom)\n",
    "            if (from_row == 1 and to_row == 0) or (from_row == 6 and to_row == 7):\n",
    "                direction_map = {\n",
    "                    0: 0,   # straight\n",
    "                    -1: 1,  # left capture\n",
    "                    1: 2    # right capture\n",
    "                }\n",
    "                dir = to_col - from_col\n",
    "                if dir in direction_map:\n",
    "                    dir_idx = direction_map[dir]\n",
    "                    piece_map = {\n",
    "                        chess.KNIGHT: 0,\n",
    "                        chess.BISHOP: 1,\n",
    "                        chess.ROOK: 2,\n",
    "                        chess.QUEEN: 3\n",
    "                    }\n",
    "                    piece_idx = piece_map[move.promotion]\n",
    "                    plane = 64 + dir_idx * 4 + piece_idx\n",
    "                    return (from_row * 8 + from_col) * 73 + plane\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid promotion direction for move: {move}, delta_col: {dir}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid promotion position for move: {move}, from: {from_row},{from_col} to: {to_row},{to_col}\")\n",
    "\n",
    "        raise ValueError(f\"Invalid or unsupported move for AlphaZero mapping: {move}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34a4fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloSearchTree:\n",
    "    def __init__(self, evaluator, simulations=800, c_puct=1.0):\n",
    "        self.evaluator = evaluator\n",
    "        self.simulations = simulations\n",
    "        self.c_puct = c_puct\n",
    "\n",
    "    def run(self, state: ChessState, history):\n",
    "        root = MonteCarloTreeNode(state=state)\n",
    "        move_priors, value = self.evaluator.evaluate(history)\n",
    "        root.expand(move_priors)\n",
    "        for _ in range(self.simulations):\n",
    "            node = root\n",
    "            path = [node]\n",
    "            current_history = history.copy()\n",
    "            while node.is_expanded():\n",
    "                move, node = self.select(node)\n",
    "                path.append(node)\n",
    "                current_state = node.state.clone()\n",
    "                current_history.append(current_state)\n",
    "                current_history = current_history[-8:]  # Keep last 8 states\n",
    "            if not node.state.is_terminal():\n",
    "                move_priors, value = self.evaluator.evaluate(current_history)\n",
    "                node.expand(move_priors)\n",
    "            self.backpropagate(path, value)\n",
    "        return self.select_action(root)\n",
    "\n",
    "    def select(self, node):\n",
    "        best_score = -float(\"inf\")\n",
    "        best_move = None\n",
    "        best_child = None\n",
    "        for move, child in node.children.items():\n",
    "            ucb = self.ucb_score(node, child)\n",
    "            if ucb > best_score:\n",
    "                best_score = ucb\n",
    "                best_move = move\n",
    "                best_child = child\n",
    "        return best_move, best_child\n",
    "\n",
    "    def ucb_score(self, parent, child):\n",
    "        q = child.value()\n",
    "        u = self.c_puct * child.prior * (np.sqrt(parent.visits) / (1 + child.visits))\n",
    "        return q + u\n",
    "\n",
    "    def backpropagate(self, path, value):\n",
    "        for node in reversed(path):\n",
    "            node.visits += 1\n",
    "            node.value_sum += value\n",
    "            value = -value\n",
    "\n",
    "    def select_action(self, root):\n",
    "        move_visits = [(move, child.visits) for move, child in root.children.items()]\n",
    "        move_visits.sort(key=lambda x: x[1], reverse=True)\n",
    "        return move_visits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97149289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from stockfish import Stockfish\n",
    "import random\n",
    "\n",
    "# SELF PLAY BICHASS\n",
    "class AlphaZeroTrainer:\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu', replay_buffer_size=1000000):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.evaluator = Evaluator(model, device)\n",
    "        self.mcts = MonteCarloSearchTree(self.evaluator, simulations=800, c_puct=1.0)\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        self.games_played = 0\n",
    "        self.checkpoint_dir = \"checkpoint\"\n",
    "        self.training_dir = \"training\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.training_dir, exist_ok=True)\n",
    "\n",
    "    def save_training_data(self, game_data, game_number):\n",
    "        filename = os.path.join(self.training_dir, f\"training_game_{game_number}.h5\")\n",
    "        with h5py.File(filename, 'w') as f:\n",
    "            for i, (state_tensor, policy_target, value_target) in enumerate(game_data):\n",
    "                game_group = f.create_group(f\"move_{i}\")\n",
    "                game_group.create_dataset('state', data=state_tensor)\n",
    "                game_group.create_dataset('policy', data=policy_target)\n",
    "                game_group.create_dataset('value', data=value_target)\n",
    "\n",
    "    def load_training_data(self):\n",
    "        training_files = [f for f in os.listdir(self.training_dir) if f.endswith('.h5')]\n",
    "        if not training_files:\n",
    "            return\n",
    "        for file in training_files:\n",
    "            with h5py.File(os.path.join(self.training_dir, file), 'r') as f:\n",
    "                for move_key in f.keys():\n",
    "                    move_group = f[move_key]\n",
    "                    state = move_group['state'][:]\n",
    "                    policy = move_group['policy'][:]\n",
    "                    value = move_group['value'][()]\n",
    "                    self.replay_buffer.append((state, policy, value))\n",
    "        print(f\"Loaded {len(training_files)} training files into replay buffer\")\n",
    "\n",
    "    def self_play(self, num_games=100):\n",
    "        for game in range(num_games):\n",
    "            state = ChessState(chess.Board())\n",
    "            history = [state.clone()]\n",
    "            game_data = []\n",
    "            while not state.is_terminal():\n",
    "                move = self.mcts.run(state, history)\n",
    "                root = MonteCarloTreeNode(state)\n",
    "                move_priors, _ = self.evaluator.evaluate(history)\n",
    "                root.expand(move_priors)\n",
    "                for _ in range(self.mcts.simulations):\n",
    "                    node = root\n",
    "                    path = [node]\n",
    "                    current_history = history.copy()\n",
    "                    while node.is_expanded():\n",
    "                        move_sim, node = self.mcts.select(node)\n",
    "                        path.append(node)\n",
    "                        current_state = node.state.clone()\n",
    "                        current_history.append(current_state)\n",
    "                        current_history = current_history[-8:]\n",
    "                    if not node.state.is_terminal():\n",
    "                        move_priors_sim, value = self.evaluator.evaluate(current_history)\n",
    "                        node.expand(move_priors_sim)\n",
    "                    self.mcts.backpropagate(path, value)\n",
    "                policy_target = np.zeros(4672)\n",
    "                total_visits = sum(child.visits for child in root.children.values())\n",
    "                for move, child in root.children.items():\n",
    "                    idx = self.evaluator.move_to_index(move)\n",
    "                    policy_target[idx] = child.visits / total_visits\n",
    "                game_data.append((boards_to_tensor(history), policy_target, None))\n",
    "                state.apply_move(move)\n",
    "                history.append(state.clone())\n",
    "                history = history[-8:]\n",
    "            result = state.result()\n",
    "            value = 1.0 if result == '1-0' else -1.0 if result == '0-1' else 0.0\n",
    "            for i in range(len(game_data)):\n",
    "                state_tensor, policy_target, _ = game_data[i]\n",
    "                game_data[i] = (state_tensor, policy_target, value if i % 2 == 0 else -value)\n",
    "            self.replay_buffer.extend(game_data)\n",
    "            self.save_training_data(game_data, self.games_played + 1)\n",
    "            self.games_played += 1\n",
    "            print(f\"Completed game {self.games_played}\")\n",
    "            if self.games_played % 100 == 0:\n",
    "                checkpoint_name = os.path.join(self.checkpoint_dir, f\"model_game_{self.games_played}.h5\")\n",
    "                self.model.save_to_hdf5(checkpoint_name)\n",
    "            if self.games_played % 10 == 0:\n",
    "                self.train(epochs=1, batch_size=128)\n",
    "\n",
    "    def train(self, epochs=1, batch_size=128):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            if len(self.replay_buffer) < batch_size:\n",
    "                continue\n",
    "            samples = random.sample(self.replay_buffer, batch_size)\n",
    "            state_tensors = torch.from_numpy(np.stack([s[0] for s in samples])).to(self.device)\n",
    "            policy_targets = torch.from_numpy(np.stack([s[1] for s in samples])).to(self.device)\n",
    "            value_targets = torch.tensor([s[2] for s in samples], dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            value, policy_logits = self.model(state_tensors)\n",
    "            \n",
    "            # Debug: Print shapes\n",
    "            print(f\"policy_logits shape: {policy_logits.shape}\")  # Should be [128, 4672]\n",
    "            print(f\"value shape: {value.shape}\")                # Should be [128, 1]\n",
    "            print(f\"value_targets shape: {value_targets.shape}\") # Should be [128]\n",
    "            \n",
    "            policy_log_probs = F.log_softmax(policy_logits, dim=1)\n",
    "            policy_loss = F.kl_div(policy_log_probs, policy_targets, reduction='batchmean')\n",
    "            value_loss = F.mse_loss(value.squeeze(), value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}, Policy Loss: {policy_loss.item():.4f}, Value Loss: {value_loss.item():.4f}\")\n",
    "\n",
    "    def evaluate_against_stockfish(self, stockfish_path, num_games=10, stockfish_depth=10):\n",
    "        stockfish = Stockfish(path=stockfish_path)\n",
    "        stockfish.set_depth(stockfish_depth)\n",
    "        results = {'win': 0, 'loss': 0, 'draw': 0}\n",
    "        self.model.eval()\n",
    "        for game in range(num_games):\n",
    "            board = chess.Board()\n",
    "            state = ChessState(board)\n",
    "            history = [state.clone()]\n",
    "            while not state.is_terminal():\n",
    "                if state.board.turn == chess.WHITE:\n",
    "                    move = self.mcts.run(state, history)\n",
    "                else:\n",
    "                    stockfish.set_fen_position(state.board.fen())\n",
    "                    move = chess.Move.from_uci(stockfish.get_best_move())\n",
    "                state.apply_move(move)\n",
    "                history.append(state.clone())\n",
    "                history = history[-8:]\n",
    "            result = state.result()\n",
    "            if result == '1-0':\n",
    "                results['win'] += 1\n",
    "            elif result == '0-1':\n",
    "                results['loss'] += 1\n",
    "            else:\n",
    "                results['draw'] += 1\n",
    "        print(f\"Evaluation results: Wins={results['win']}, Losses={results['loss']}, Draws={results['draw']}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ed210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. Starting fresh.\n",
      "Loaded 10 training files into replay buffer\n",
      "Training model with loaded data...\n",
      "policy_logits shape: torch.Size([128, 4672])\n",
      "value shape: torch.Size([128, 1])\n",
      "value_targets shape: torch.Size([128])\n",
      "Epoch 1, Loss: 7.7843, Policy Loss: 7.7434, Value Loss: 0.0408\n",
      "Self-playing 100 games (games 1 to 100)...\n"
     ]
    }
   ],
   "source": [
    "# Train it\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = DeepNeuralNetwork(in_channels=103, out_channels=256, num_residual=10, hidden_size=256).to(device)\n",
    "    trainer = AlphaZeroTrainer(model, device)\n",
    "\n",
    "    # Load latest checkpoint if available\n",
    "    checkpoint_files = [f for f in os.listdir(trainer.checkpoint_dir) if f.endswith('.h5')]\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        checkpoint_path = os.path.join(trainer.checkpoint_dir, latest_checkpoint)\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        model.load_from_hdf5(checkpoint_path)\n",
    "        trainer.games_played = int(latest_checkpoint.split('_')[-1].split('.')[0])\n",
    "    else:\n",
    "        print(\"No checkpoints found. Starting fresh.\")\n",
    "\n",
    "    # Load training data and train if available\n",
    "    trainer.load_training_data()\n",
    "    if len(trainer.replay_buffer) > 0:\n",
    "        print(\"Training model with loaded data...\")\n",
    "        trainer.train(epochs=1, batch_size=128)\n",
    "\n",
    "    total_games = 1000\n",
    "    step = 100\n",
    "    games_played = trainer.games_played\n",
    "\n",
    "    while games_played < total_games:\n",
    "        batch_games = min(step, total_games - games_played)\n",
    "        print(f\"Self-playing {batch_games} games (games {games_played + 1} to {games_played + batch_games})...\")\n",
    "        trainer.self_play(num_games=batch_games)\n",
    "        games_played += batch_games\n",
    "\n",
    "    # Evaluate against Stockfish after training\n",
    "    stockfish_path = \".venv/Lib/site-packages/stockfish\"\n",
    "    trainer.evaluate_against_stockfish(stockfish_path, num_games=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
