{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b822c53d",
   "metadata": {},
   "source": [
    "In this notebook we are going through steps to setup a simple chess board AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b57376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chess in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (1.11.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3996ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "\n",
    "# Simple chess board for functioning my AI\n",
    "board = chess.Board()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a0a95",
   "metadata": {},
   "source": [
    "We can use this for our agents playground. Time to setup the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f790ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stockfish in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (3.28.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install stockfish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1eb7bf",
   "metadata": {},
   "source": [
    "Now we have the opponent to test ground truth.\n",
    "For making an AlphaZero model, we will need it in the very late stage - when the model is almost done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb1da75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (2.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c3c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We build the ChessState for AlphaZero to evaluate first\n",
    "class ChessState:\n",
    "    def __init__(self, board: chess.Board):\n",
    "        self.board = board\n",
    "\n",
    "    def clone(self):\n",
    "        return ChessState(self.board.copy())\n",
    "\n",
    "    def apply_move(self, move: chess.Move):\n",
    "        self.board.push(move)\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.board.is_game_over()\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.board.legal_moves)\n",
    "\n",
    "    def result(self):\n",
    "        return self.board.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee0076",
   "metadata": {},
   "source": [
    "Next we focus on Deep Neural Network\n",
    "This network focus on learning the probability of the next move -> the policy of the game, the value for the current player -> the game ultimate winner following this move\n",
    "\n",
    "Input (C, 8, 8) (game state)\n",
    "↓\n",
    "Conv Layer (Conv 3x3 x 256, BN, ReLU)\n",
    "↓\n",
    "Residual Layer × N (Conv 3x3 x 256, BN, ReLU, Conv 3x3 x 256, BN, Skip Connection, ReLU) # This skip is important to help model learn, avoid desc grad\n",
    "↓\n",
    "↓-------------------↓\n",
    "|                   |\n",
    "Policy Head         Value Head\n",
    "Conv 1×1 x 2        Conv 1×1\n",
    "BN, ReLU            BN, ReLU\n",
    "FC (to 4672)        FC (hidden, 256) → ReLU → FC (1) → tanh\n",
    "\n",
    "Now there might be some confoosion (kinda) around 4672.\n",
    "4672 = 8x8x73\n",
    "= 8x8 x (56 directional offsets + 8 knight moves + 9 promotions)\n",
    "Now you may be more CONFOOSION, extra promotion just dropped out of somewhere\n",
    "This is for strong model to learn underpromotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0636810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from torch) (80.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: h5py in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in d:\\codespace\\python\\chess with alphazero\\.venv\\lib\\site-packages (from h5py) (2.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f09432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just jump directly into making layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "# Input Convolutional Layer\n",
    "class InputConvolutionalLayer(nn.Module):\n",
    "    def __init__(self, in_channels=103, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def load_from_hdf5(self, h5_group):\n",
    "        self.conv.weight.data = torch.tensor(h5_group['conv_weight'][:])\n",
    "        self.conv.bias.data = torch.tensor(h5_group['conv_bias'][:])\n",
    "        self.bn.weight.data = torch.tensor(h5_group['bn_weight'][:])\n",
    "        self.bn.bias.data = torch.tensor(h5_group['bn_bias'][:])\n",
    "        self.bn.running_mean.data = torch.tensor(h5_group['bn_running_mean'][:])\n",
    "        self.bn.running_var.data = torch.tensor(h5_group['bn_running_var'][:])\n",
    "    \n",
    "# Residual Layer\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, channels=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2= nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2= nn.BatchNorm2d(num_features=channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        residual = self.conv1(residual)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.relu1(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "        residual = residual + x\n",
    "        residual = self.relu2(residual)\n",
    "        return residual\n",
    "    \n",
    "    def load_from_hdf5(self, h5_group):\n",
    "        self.conv1.weight.data = torch.tensor(h5_group['conv1_weight'][:])\n",
    "        self.conv1.bias.data = torch.tensor(h5_group['conv1_bias'][:])\n",
    "        self.bn1.weight.data = torch.tensor(h5_group['bn1_weight'][:])\n",
    "        self.bn1.bias.data = torch.tensor(h5_group['bn1_bias'][:])\n",
    "        self.bn1.running_mean.data = torch.tensor(h5_group['bn1_running_mean'][:])\n",
    "        self.bn1.running_var.data = torch.tensor(h5_group['bn1_running_var'][:])\n",
    "        self.conv2.weight.data = torch.tensor(h5_group['conv2_weight'][:])\n",
    "        self.conv2.bias.data = torch.tensor(h5_group['conv2_bias'][:])\n",
    "        self.bn2.weight.data = torch.tensor(h5_group['bn2_weight'][:])\n",
    "        self.bn2.bias.data = torch.tensor(h5_group['bn2_bias'][:])\n",
    "        self.bn2.running_mean.data = torch.tensor(h5_group['bn2_running_mean'][:])\n",
    "        self.bn2.running_var.data = torch.tensor(h5_group['bn2_running_var'][:])\n",
    "\n",
    "# Value Head\n",
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size):\n",
    "        super(ValueHead, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(1)\n",
    "        self.fcl1 = nn.Linear(64, hidden_size)  # 8x8 board\n",
    "        self.fcl2 = nn.Linear(hidden_size, 1)   # Fix: Output a single value\n",
    "        self.activation = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.bn(self.conv(x)))\n",
    "        x = x.view(-1, 64)  # Flatten\n",
    "        x = self.activation(self.fcl1(x))\n",
    "        x = self.tanh(self.fcl2(x))  # Output shape: [batch_size, 1]\n",
    "        return x\n",
    "    \n",
    "    def load_from_hdf5(self, h5_group):\n",
    "        self.conv.weight.data = torch.tensor(h5_group['conv_weight'][:])\n",
    "        self.conv.bias.data = torch.tensor(h5_group['conv_bias'][:])\n",
    "        self.bn.weight.data = torch.tensor(h5_group['bn_weight'][:])\n",
    "        self.bn.bias.data = torch.tensor(h5_group['bn_bias'][:])\n",
    "        self.bn.running_mean.data = torch.tensor(h5_group['bn_running_mean'][:])\n",
    "        self.bn.running_var.data = torch.tensor(h5_group['bn_running_var'][:])\n",
    "        self.fcl1.weight.data = torch.tensor(h5_group['fcl1_weight'][:])\n",
    "        self.fcl1.bias.data = torch.tensor(h5_group['fcl1_bias'][:])\n",
    "        self.fcl2.weight.data = torch.tensor(h5_group['fcl2_weight'][:])\n",
    "        self.fcl2.bias.data = torch.tensor(h5_group['fcl2_bias'][:])\n",
    "    \n",
    "# Policy Head\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0)\n",
    "        self.bn = nn.BatchNorm2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fcl = nn.Linear(in_features = 8 * 8 * 2, out_features = 8 * 8 * (56 + 8 + 9))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fcl(x)\n",
    "        return x\n",
    "    \n",
    "    def load_from_hdf5(self, h5_group):\n",
    "        self.conv.weight.data = torch.tensor(h5_group['conv_weight'][:])\n",
    "        self.conv.bias.data = torch.tensor(h5_group['conv_bias'][:])\n",
    "        self.bn.weight.data = torch.tensor(h5_group['bn_weight'][:])\n",
    "        self.bn.bias.data = torch.tensor(h5_group['bn_bias'][:])\n",
    "        self.bn.running_mean.data = torch.tensor(h5_group['bn_running_mean'][:])\n",
    "        self.bn.running_var.data = torch.tensor(h5_group['bn_running_var'][:])\n",
    "        self.fcl.weight.data = torch.tensor(h5_group['fcl_weight'][:])\n",
    "        self.fcl.bias.data = torch.tensor(h5_group['fcl_bias'][:])\n",
    "    \n",
    "# Combination is power\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=103, out_channels=256, num_residual=20, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.conv = InputConvolutionalLayer(in_channels=in_channels, out_channels=out_channels)\n",
    "        self.residual_list = nn.Sequential(*[ResidualLayer(channels=out_channels) for _ in range(20)])\n",
    "        self.value_head = ValueHead(in_channels=out_channels, hidden_size=hidden_size)\n",
    "        self.policy_head = PolicyHead(channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.residual_list(x)\n",
    "        # Clone x for both head\n",
    "        value_x = x.clone()\n",
    "        policy_x = x\n",
    "        return self.value_head(value_x), self.policy_head(policy_x)\n",
    "    \n",
    "    def save_to_hdf5(self, filename):\n",
    "        with h5py.File(filename, 'w') as f:\n",
    "            # Save InputConvolutionalLayer\n",
    "            conv_group = f.create_group('conv')\n",
    "            conv_group.create_dataset('conv_weight', data=self.conv.conv.weight.data.cpu().numpy())\n",
    "            conv_group.create_dataset('conv_bias', data=self.conv.conv.bias.data.cpu().numpy())\n",
    "            conv_group.create_dataset('bn_weight', data=self.conv.bn.weight.data.cpu().numpy())\n",
    "            conv_group.create_dataset('bn_bias', data=self.conv.bn.bias.data.cpu().numpy())\n",
    "            conv_group.create_dataset('bn_running_mean', data=self.conv.bn.running_mean.data.cpu().numpy())\n",
    "            conv_group.create_dataset('bn_running_var', data=self.conv.bn.running_var.data.cpu().numpy())\n",
    "\n",
    "            # Save Residual Layers\n",
    "            for i, residual in enumerate(self.residual_list):\n",
    "                res_group = f.create_group(f'residual_{i}')\n",
    "                res_group.create_dataset('conv1_weight', data=residual.conv1.weight.data.cpu().numpy())\n",
    "                res_group.create_dataset('conv1_bias', data=residual.conv1.bias.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn1_weight', data=residual.bn1.weight.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn1_bias', data=residual.bn1.bias.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn1_running_mean', data=residual.bn1.running_mean.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn1_running_var', data=residual.bn1.running_var.data.cpu().numpy())\n",
    "                res_group.create_dataset('conv2_weight', data=residual.conv2.weight.data.cpu().numpy())\n",
    "                res_group.create_dataset('conv2_bias', data=residual.conv2.bias.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn2_weight', data=residual.bn2.weight.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn2_bias', data=residual.bn2.bias.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn2_running_mean', data=residual.bn2.running_mean.data.cpu().numpy())\n",
    "                res_group.create_dataset('bn2_running_var', data=residual.bn2.running_var.data.cpu().numpy())\n",
    "\n",
    "            # Save Value Head\n",
    "            value_group = f.create_group('value_head')\n",
    "            value_group.create_dataset('conv_weight', data=self.value_head.conv.weight.data.cpu().numpy())\n",
    "            value_group.create_dataset('conv_bias', data=self.value_head.conv.bias.data.cpu().numpy())\n",
    "            value_group.create_dataset('bn_weight', data=self.value_head.bn.weight.data.cpu().numpy())\n",
    "            value_group.create_dataset('bn_bias', data=self.value_head.bn.bias.data.cpu().numpy())\n",
    "            value_group.create_dataset('bn_running_mean', data=self.value_head.bn.running_mean.data.cpu().numpy())\n",
    "            value_group.create_dataset('bn_running_var', data=self.value_head.bn.running_var.data.cpu().numpy())\n",
    "            value_group.create_dataset('fcl1_weight', data=self.value_head.fcl1.weight.data.cpu().numpy())\n",
    "            value_group.create_dataset('fcl1_bias', data=self.value_head.fcl1.bias.data.cpu().numpy())\n",
    "            value_group.create_dataset('fcl2_weight', data=self.value_head.fcl2.weight.data.cpu().numpy())\n",
    "            value_group.create_dataset('fcl2_bias', data=self.value_head.fcl2.bias.data.cpu().numpy())\n",
    "\n",
    "            # Save Policy Head\n",
    "            policy_group = f.create_group('policy_head')\n",
    "            policy_group.create_dataset('conv_weight', data=self.policy_head.conv.weight.data.cpu().numpy())\n",
    "            policy_group.create_dataset('conv_bias', data=self.policy_head.conv.bias.data.cpu().numpy())\n",
    "            policy_group.create_dataset('bn_weight', data=self.policy_head.bn.weight.data.cpu().numpy())\n",
    "            policy_group.create_dataset('bn_bias', data=self.policy_head.bn.bias.data.cpu().numpy())\n",
    "            policy_group.create_dataset('bn_running_mean', data=self.policy_head.bn.running_mean.data.cpu().numpy())\n",
    "            policy_group.create_dataset('bn_running_var', data=self.policy_head.bn.running_var.data.cpu().numpy())\n",
    "            policy_group.create_dataset('fcl_weight', data=self.policy_head.fcl.weight.data.cpu().numpy())\n",
    "            policy_group.create_dataset('fcl_bias', data=self.policy_head.fcl.bias.data.cpu().numpy())\n",
    "\n",
    "    def load_from_hdf5(self, filename):\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            self.conv.load_from_hdf5(f['conv'])\n",
    "            for i, residual in enumerate(self.residual_list):\n",
    "                residual.load_from_hdf5(f[f'residual_{i}'])\n",
    "            self.value_head.load_from_hdf5(f['value_head'])\n",
    "            self.policy_head.load_from_hdf5(f['policy_head'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88655da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost perfect, but wait: The input is not ready\n",
    "# Function to convert board list to tensor\n",
    "def boards_to_tensor(history):\n",
    "    tensor = np.zeros((103, 8, 8), dtype=np.float32)\n",
    "    piece_planes = {\n",
    "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11,\n",
    "    }\n",
    "    # Piece planes (up to 8 timesteps)\n",
    "    for t, chess_state in enumerate(history[-8:]):  # Take last 8 states\n",
    "        board = chess_state.board\n",
    "        offset = t * 12\n",
    "        for square, piece in board.piece_map().items():\n",
    "            row = 7 - chess.square_rank(square)\n",
    "            col = chess.square_file(square)\n",
    "            plane = offset + piece_planes[piece.symbol()]\n",
    "            tensor[plane, row, col] = 1\n",
    "    latest_board = history[-1].board\n",
    "    tensor[96, :, :] = int(latest_board.turn)\n",
    "    tensor[97, :, :] = int(latest_board.has_kingside_castling_rights(chess.WHITE))\n",
    "    tensor[98, :, :] = int(latest_board.has_queenside_castling_rights(chess.WHITE))\n",
    "    tensor[99, :, :] = int(latest_board.has_kingside_castling_rights(chess.BLACK))\n",
    "    tensor[100, :, :] = int(latest_board.has_queenside_castling_rights(chess.BLACK))\n",
    "    tensor[101, :, :] = latest_board.halfmove_clock / 100.0\n",
    "    tensor[102, :, :] = latest_board.fullmove_number / 100.0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4604a",
   "metadata": {},
   "source": [
    "So far so good.\n",
    "We are gonna move to Monte Carlo Tree Search this time\n",
    "For overview, this is our tree setup: From a node state s, we will have action node a contains:\n",
    "- Visit count N(s, a)\n",
    "- Total value W(s, a)\n",
    "- Mean action value Q(s, a) = W(s, a) / N(s, a)\n",
    "- Prio probability P(s, a)\n",
    "Traversal will use PUCT formula: a_t = argmax(Q(s, a) + U(s, a))\n",
    "with U(s, a) = c_puct * P(s, a) * sqrt(SUM N(s, b)) / (1 + N(s, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd64a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloTreeNode:\n",
    "    def __init__(self, state : ChessState, parent=None, prior=0.0):\n",
    "        self.state = state              # ChessState\n",
    "        self.parent = parent            # MonteCarloTreeNode\n",
    "        self.prior = prior              # from policy network\n",
    "        self.children = {}              # move (chess.Move) -> MonteCarloTreeNode\n",
    "        self.visits = 0\n",
    "        self.value_sum = 0.0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        return self.value_sum / self.visits if self.visits > 0 else 0\n",
    "\n",
    "    def expand(self, move_priors):\n",
    "        for move, prior in move_priors.items():\n",
    "            next_state = self.state.clone()\n",
    "            next_state.apply_move(move)\n",
    "            self.children[move] = MonteCarloTreeNode(next_state, parent=self, prior=prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e7a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def evaluate(self, history):\n",
    "        tensor = boards_to_tensor(history)\n",
    "        input_tensor = torch.tensor(tensor).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            value, policy_logits= self.model(input_tensor)\n",
    "\n",
    "        policy = F.softmax(policy_logits, dim=1).squeeze(0).cpu().numpy()\n",
    "        value = value.item()\n",
    "\n",
    "        # Map policy logits to legal moves\n",
    "        legal_moves = history[-1].legal_moves()\n",
    "        move_priors = {}\n",
    "        for move in legal_moves:\n",
    "            idx = self.move_to_index(move)  # needs matching encoding\n",
    "            move_priors[move] = policy[idx]\n",
    "\n",
    "        return move_priors, value\n",
    "\n",
    "    def move_to_index(self, move: chess.Move) -> int:\n",
    "        from_square = move.from_square\n",
    "        to_square = move.to_square\n",
    "        from_row = 7 - chess.square_rank(from_square)\n",
    "        from_col = chess.square_file(from_square)\n",
    "        to_row = 7 - chess.square_rank(to_square)\n",
    "        to_col = chess.square_file(to_square)\n",
    "\n",
    "        delta_row = to_row - from_row\n",
    "        delta_col = to_col - from_col\n",
    "\n",
    "        # === 1. Directional moves: 8 directions × 7 steps = 56 planes ===\n",
    "        if move.promotion is None:\n",
    "            directions = [\n",
    "                (0, 1),   # → right\n",
    "                (0, -1),  # ← left\n",
    "                (1, 0),   # ↑ up\n",
    "                (-1, 0),  # ↓ down\n",
    "                (1, 1),   # ↗\n",
    "                (1, -1),  # ↖\n",
    "                (-1, 1),  # ↘\n",
    "                (-1, -1)  # ↙\n",
    "            ]\n",
    "\n",
    "            for dir_idx, (dr, dc) in enumerate(directions):\n",
    "                for step in range(1, 8):  # up to 7 squares\n",
    "                    if delta_row == dr * step and delta_col == dc * step:\n",
    "                        plane = dir_idx * 7 + (step - 1)\n",
    "                        return (from_row * 8 + from_col) * 73 + plane\n",
    "\n",
    "        # === 2. Knight moves: 8 planes ===\n",
    "        knight_deltas = [\n",
    "            (2, 1), (2, -1), (-2, 1), (-2, -1),\n",
    "            (1, 2), (1, -2), (-1, 2), (-1, -2)\n",
    "        ]\n",
    "        for i, (dr, dc) in enumerate(knight_deltas):\n",
    "            if delta_row == dr and delta_col == dc:\n",
    "                return (from_row * 8 + from_col) * 73 + 56 + i\n",
    "\n",
    "        # === 3. Promotion moves: 3 directions × 4 pieces (knight, bishop, rook, queen) = 12 planes (64–75) ===\n",
    "        if move.promotion in {chess.KNIGHT, chess.BISHOP, chess.ROOK, chess.QUEEN}:\n",
    "            # Check if it's a valid promotion move (pawns on second-to-last rank)\n",
    "            # For white: from_row is 1 (second rank from top)\n",
    "            # For black: from_row is 6 (second rank from bottom)\n",
    "            if (from_row == 1 and to_row == 0) or (from_row == 6 and to_row == 7):\n",
    "                direction_map = {\n",
    "                    0: 0,   # straight\n",
    "                    -1: 1,  # left capture\n",
    "                    1: 2    # right capture\n",
    "                }\n",
    "                dir = to_col - from_col\n",
    "                if dir in direction_map:\n",
    "                    dir_idx = direction_map[dir]\n",
    "                    piece_map = {\n",
    "                        chess.KNIGHT: 0,\n",
    "                        chess.BISHOP: 1,\n",
    "                        chess.ROOK: 2,\n",
    "                        chess.QUEEN: 3\n",
    "                    }\n",
    "                    piece_idx = piece_map[move.promotion]\n",
    "                    plane = 64 + dir_idx * 4 + piece_idx\n",
    "                    return (from_row * 8 + from_col) * 73 + plane\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid promotion direction for move: {move}, delta_col: {dir}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid promotion position for move: {move}, from: {from_row},{from_col} to: {to_row},{to_col}\")\n",
    "\n",
    "        raise ValueError(f\"Invalid or unsupported move for AlphaZero mapping: {move}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a4fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloSearchTree:\n",
    "    def __init__(self, evaluator, simulations=800, c_puct=1.0):\n",
    "        self.evaluator = evaluator\n",
    "        self.simulations = simulations\n",
    "        self.c_puct = c_puct\n",
    "\n",
    "    def run(self, state: ChessState, history):\n",
    "        root = MonteCarloTreeNode(state=state)\n",
    "        move_priors, value = self.evaluator.evaluate(history)\n",
    "        root.expand(move_priors)\n",
    "        for _ in range(self.simulations):\n",
    "            node = root\n",
    "            path = [node]\n",
    "            current_history = history.copy()\n",
    "            while node.is_expanded():\n",
    "                move, node = self.select(node)\n",
    "                path.append(node)\n",
    "                current_state = node.state.clone()\n",
    "                current_history.append(current_state)\n",
    "                current_history = current_history[-8:]  # Keep last 8 states\n",
    "            if not node.state.is_terminal():\n",
    "                move_priors, value = self.evaluator.evaluate(current_history)\n",
    "                node.expand(move_priors)\n",
    "            self.backpropagate(path, value)\n",
    "        return self.select_action(root)\n",
    "\n",
    "    def select(self, node):\n",
    "        best_score = -float(\"inf\")\n",
    "        best_move = None\n",
    "        best_child = None\n",
    "        for move, child in node.children.items():\n",
    "            ucb = self.ucb_score(node, child)\n",
    "            if ucb > best_score:\n",
    "                best_score = ucb\n",
    "                best_move = move\n",
    "                best_child = child\n",
    "        return best_move, best_child\n",
    "\n",
    "    def ucb_score(self, parent, child):\n",
    "        q = child.value()\n",
    "        u = self.c_puct * child.prior * (np.sqrt(parent.visits) / (1 + child.visits))\n",
    "        return q + u\n",
    "\n",
    "    def backpropagate(self, path, value):\n",
    "        for node in reversed(path):\n",
    "            node.visits += 1\n",
    "            node.value_sum += value\n",
    "            value = -value\n",
    "\n",
    "    def select_action(self, root):\n",
    "        move_visits = [(move, child.visits) for move, child in root.children.items()]\n",
    "        move_visits.sort(key=lambda x: x[1], reverse=True)\n",
    "        return move_visits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97149289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from stockfish import Stockfish\n",
    "import random\n",
    "\n",
    "# SELF PLAY BICHASS\n",
    "class AlphaZeroTrainer:\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu', replay_buffer_size=1000000):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.evaluator = Evaluator(model, device)\n",
    "        self.mcts = MonteCarloSearchTree(self.evaluator, simulations=100, c_puct=1.0)\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        self.games_played = 0\n",
    "        self.checkpoint_dir = \"checkpoint\"\n",
    "        self.training_dir = \"training\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(self.training_dir, exist_ok=True)\n",
    "\n",
    "    def save_training_data(self, game_data, game_number):\n",
    "        filename = os.path.join(self.training_dir, f\"training_game_{game_number}.h5\")\n",
    "        with h5py.File(filename, 'w') as f:\n",
    "            for i, (state_tensor, policy_target, value_target) in enumerate(game_data):\n",
    "                game_group = f.create_group(f\"move_{i}\")\n",
    "                game_group.create_dataset('state', data=state_tensor)\n",
    "                game_group.create_dataset('policy', data=policy_target)\n",
    "                game_group.create_dataset('value', data=value_target)\n",
    "\n",
    "    def load_training_data(self):\n",
    "        training_files = [f for f in os.listdir(self.training_dir) if f.endswith('.h5')]\n",
    "        if not training_files:\n",
    "            return\n",
    "        for file in training_files:\n",
    "            with h5py.File(os.path.join(self.training_dir, file), 'r') as f:\n",
    "                for move_key in f.keys():\n",
    "                    move_group = f[move_key]\n",
    "                    state = move_group['state'][:]\n",
    "                    policy = move_group['policy'][:]\n",
    "                    value = move_group['value'][()]\n",
    "                    self.replay_buffer.append((state, policy, value))\n",
    "        print(f\"Loaded {len(training_files)} training files into replay buffer\")\n",
    "\n",
    "    def self_play(self, num_games=100):\n",
    "        for game in range(num_games):\n",
    "            state = ChessState(chess.Board())\n",
    "            history = [state.clone()]\n",
    "            game_data = []\n",
    "            while not state.is_terminal():\n",
    "                move = self.mcts.run(state, history)\n",
    "                root = MonteCarloTreeNode(state)\n",
    "                move_priors, _ = self.evaluator.evaluate(history)\n",
    "                root.expand(move_priors)\n",
    "                for _ in range(self.mcts.simulations):\n",
    "                    node = root\n",
    "                    path = [node]\n",
    "                    current_history = history.copy()\n",
    "                    while node.is_expanded():\n",
    "                        move_sim, node = self.mcts.select(node)\n",
    "                        path.append(node)\n",
    "                        current_state = node.state.clone()\n",
    "                        current_history.append(current_state)\n",
    "                        current_history = current_history[-8:]\n",
    "                    if not node.state.is_terminal():\n",
    "                        move_priors_sim, value = self.evaluator.evaluate(current_history)\n",
    "                        node.expand(move_priors_sim)\n",
    "                    self.mcts.backpropagate(path, value)\n",
    "                policy_target = np.zeros(4672)\n",
    "                total_visits = sum(child.visits for child in root.children.values())\n",
    "                for move, child in root.children.items():\n",
    "                    idx = self.evaluator.move_to_index(move)\n",
    "                    policy_target[idx] = child.visits / total_visits\n",
    "                game_data.append((boards_to_tensor(history), policy_target, None))\n",
    "                state.apply_move(move)\n",
    "                history.append(state.clone())\n",
    "                history = history[-8:]\n",
    "            result = state.result()\n",
    "            value = 1.0 if result == '1-0' else -1.0 if result == '0-1' else 0.0\n",
    "            for i in range(len(game_data)):\n",
    "                state_tensor, policy_target, _ = game_data[i]\n",
    "                game_data[i] = (state_tensor, policy_target, value if i % 2 == 0 else -value)\n",
    "            self.replay_buffer.extend(game_data)\n",
    "            self.save_training_data(game_data, self.games_played + 1)\n",
    "            self.games_played += 1\n",
    "            print(f\"Completed game {self.games_played}\")\n",
    "            if self.games_played % 100 == 0:\n",
    "                checkpoint_name = os.path.join(self.checkpoint_dir, f\"model_game_{self.games_played}.h5\")\n",
    "                self.model.save_to_hdf5(checkpoint_name)\n",
    "            if self.games_played % 10 == 0:\n",
    "                self.train(epochs=1, batch_size=128)\n",
    "\n",
    "    def train(self, epochs=1, batch_size=128):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            if len(self.replay_buffer) < batch_size:\n",
    "                continue\n",
    "            samples = random.sample(self.replay_buffer, batch_size)\n",
    "            state_tensors = torch.from_numpy(np.stack([s[0] for s in samples])).to(self.device)\n",
    "            policy_targets = torch.from_numpy(np.stack([s[1] for s in samples])).to(self.device)\n",
    "            value_targets = torch.tensor([s[2] for s in samples], dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            value, policy_logits = self.model(state_tensors)\n",
    "            \n",
    "            # Debug: Print shapes\n",
    "            print(f\"policy_logits shape: {policy_logits.shape}\")  # Should be [128, 4672]\n",
    "            print(f\"value shape: {value.shape}\")                # Should be [128, 1]\n",
    "            print(f\"value_targets shape: {value_targets.shape}\") # Should be [128]\n",
    "            \n",
    "            policy_log_probs = F.log_softmax(policy_logits, dim=1)\n",
    "            policy_loss = F.kl_div(policy_log_probs, policy_targets, reduction='batchmean')\n",
    "            value_loss = F.mse_loss(value.squeeze(), value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}, Policy Loss: {policy_loss.item():.4f}, Value Loss: {value_loss.item():.4f}\")\n",
    "\n",
    "    def evaluate_against_stockfish(self, stockfish_path, num_games=10, stockfish_depth=10):\n",
    "        stockfish = Stockfish(path=stockfish_path)\n",
    "        stockfish.set_depth(stockfish_depth)\n",
    "        results = {'win': 0, 'loss': 0, 'draw': 0}\n",
    "        self.model.eval()\n",
    "        for game in range(num_games):\n",
    "            board = chess.Board()\n",
    "            state = ChessState(board)\n",
    "            history = [state.clone()]\n",
    "            while not state.is_terminal():\n",
    "                if state.board.turn == chess.WHITE:\n",
    "                    move = self.mcts.run(state, history)\n",
    "                else:\n",
    "                    stockfish.set_fen_position(state.board.fen())\n",
    "                    move = chess.Move.from_uci(stockfish.get_best_move())\n",
    "                state.apply_move(move)\n",
    "                history.append(state.clone())\n",
    "                history = history[-8:]\n",
    "            result = state.result()\n",
    "            if result == '1-0':\n",
    "                results['win'] += 1\n",
    "            elif result == '0-1':\n",
    "                results['loss'] += 1\n",
    "            else:\n",
    "                results['draw'] += 1\n",
    "        print(f\"Evaluation results: Wins={results['win']}, Losses={results['loss']}, Draws={results['draw']}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ed210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest checkpoint from checkpoint\\model_game_400.h5\n",
      "No gaps in game sequence. Starting from game 414\n",
      "Self-playing 100 games (games 414 to 513)...\n",
      "Completed game 414\n"
     ]
    }
   ],
   "source": [
    "# Train it\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = DeepNeuralNetwork(in_channels=103, out_channels=256, num_residual=10, hidden_size=256).to(device)\n",
    "    trainer = AlphaZeroTrainer(model, device)\n",
    "\n",
    "    total_games = 1000\n",
    "    step = 100\n",
    "\n",
    "    checkpoint_files = [f for f in os.listdir(trainer.checkpoint_dir) if f.endswith('.h5')]\n",
    "    training_files = [f for f in os.listdir(trainer.training_dir) if f.endswith('.h5')]\n",
    "\n",
    "    # Process checkpoints to find gaps\n",
    "    if checkpoint_files:\n",
    "        # Extract game numbers from checkpoint filenames\n",
    "        game_numbers = [int(f.split('_')[-1].split('.')[0]) for f in training_files]\n",
    "        game_numbers.sort()\n",
    "        \n",
    "        # Load the latest checkpoint\n",
    "        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        checkpoint_path = os.path.join(trainer.checkpoint_dir, latest_checkpoint)\n",
    "        print(f\"Loading latest checkpoint from {checkpoint_path}\")\n",
    "        model.load_from_hdf5(checkpoint_path)\n",
    "        \n",
    "        # Find the first gap in game numbers\n",
    "        first_game = min(game_numbers)\n",
    "        max_game = max(game_numbers)\n",
    "        missing_games = [i for i in range(first_game, max_game + 1) if i not in game_numbers]\n",
    "        \n",
    "        if missing_games:\n",
    "            # Set games_played to the first missing game to fill the gap\n",
    "            trainer.games_played = missing_games[0] - 1\n",
    "            print(f\"Found gaps in game sequence. Starting from game {trainer.games_played} to fill gaps: {missing_games}\")\n",
    "        else:\n",
    "            # No gaps, start from the next game after the latest checkpoint\n",
    "            trainer.games_played = max_game - 1\n",
    "            print(f\"No gaps in game sequence. Starting from game {trainer.games_played + 1}\")\n",
    "\n",
    "    # Load training data and train if available and no checkpoints\n",
    "    elif len(training_files) > 0:\n",
    "        # Extract game numbers from checkpoint filenames\n",
    "        game_numbers = [int(f.split('_')[-1].split('.')[0]) for f in training_files]\n",
    "        game_numbers.sort()\n",
    "\n",
    "        trainer.load_training_data()\n",
    "        if len(trainer.replay_buffer) > 0:\n",
    "            print(\"Training model with loaded data...\")\n",
    "            trainer.train(epochs=1, batch_size=128)\n",
    "        trainer.games_played += len(training_files)\n",
    "\n",
    "        # Find the first gap in game numbers\n",
    "        first_game = min(game_numbers)\n",
    "        max_game = max(game_numbers)\n",
    "        missing_games = [i for i in range(first_game, max_game + 1) if i not in game_numbers]\n",
    "        \n",
    "        if missing_games:\n",
    "            # Set games_played to the first missing game to fill the gap\n",
    "            trainer.games_played = missing_games[0] - 1\n",
    "            print(f\"Found gaps in game sequence. Starting from game {trainer.games_played} to fill gaps: {missing_games}\")\n",
    "        else:\n",
    "            # No gaps, start from the next game after the latest checkpoint\n",
    "            trainer.games_played = max_game - 1\n",
    "            print(f\"No gaps in game sequence. Starting from game {trainer.games_played + 1}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No checkpoints or training data found. Starting fresh.\")\n",
    "        trainer.games_played = 0  # Initialize games_played if starting fresh\n",
    "\n",
    "    while trainer.games_played < total_games:\n",
    "        batch_games = min(step, total_games - trainer.games_played)\n",
    "        print(f\"Self-playing {batch_games} games (games {trainer.games_played + 1} to {trainer.games_played + batch_games})...\")\n",
    "        trainer.self_play(num_games=batch_games)\n",
    "        trainer.games_played += batch_games\n",
    "\n",
    "    # Evaluate against Stockfish after training\n",
    "    stockfish_path = \".venv/Lib/site-packages/stockfish\"\n",
    "    trainer.evaluate_against_stockfish(stockfish_path, num_games=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46cecb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from checkpoint\\model_game_400.h5\n",
      "AI plays as White against Stockfish.\n",
      "Game over. Result: 0-1\n",
      "Game result logged to logs\\game_20250522_190956.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import chess\n",
    "import os\n",
    "import random\n",
    "from stockfish import Stockfish\n",
    "from datetime import datetime\n",
    "\n",
    "def get_valid_user_move(board):\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Enter your move (e.g., e2e4) or 'resign' to concede: \").strip().lower()\n",
    "            if user_input == 'resign':\n",
    "                return None\n",
    "            move = chess.Move.from_uci(user_input)\n",
    "            if move in board.legal_moves:\n",
    "                return move\n",
    "            else:\n",
    "                print(\"Invalid move. Please enter a legal move (e.g., e2e4).\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input format. Use UCI notation (e.g., e2e4).\")\n",
    "\n",
    "def print_board(board):\n",
    "    piece_map = {\n",
    "        'P': '♙', 'N': '♘', 'B': '♗', 'R': '♖', 'Q': '♕', 'K': '♔',\n",
    "        'p': '♟', 'n': '♞', 'b': '♝', 'r': '♜', 'q': '♛', 'k': '♚'\n",
    "    }\n",
    "    print(\"\\n   +-----------------+\")\n",
    "    for rank in range(7, -1, -1):\n",
    "        print(f\" {rank + 1} |\", end=\" \")\n",
    "        for file in range(8):\n",
    "            square = chess.square(file, rank)\n",
    "            piece = board.piece_at(square)\n",
    "            symbol = piece_map.get(piece.symbol(), '.') if piece else '.'\n",
    "            print(symbol, end=\" \")\n",
    "        print(\"|\")\n",
    "    print(\"   +-----------------+\")\n",
    "    print(\"     a b c d e f g h\\n\")\n",
    "\n",
    "def log_game_result(log_dir, opponent, ai_side, result, moves):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = os.path.join(log_dir, f\"game_{timestamp}.txt\")\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"Opponent: {opponent}\\n\")\n",
    "        f.write(f\"AI Side: {'White' if ai_side == chess.WHITE else 'Black'}\\n\")\n",
    "        f.write(f\"Result: {result}\\n\")\n",
    "        f.write(\"Moves:\\n\")\n",
    "        for i, move in enumerate(moves, 1):\n",
    "            f.write(f\"{i}. {move.uci()}\\n\")\n",
    "    print(f\"Game result logged to {filename}\")\n",
    "\n",
    "def play_against_user(trainer, ai_side, log_dir):\n",
    "    board = chess.Board()\n",
    "    state = ChessState(board)\n",
    "    history = [state.clone()]\n",
    "    moves = []\n",
    "    \n",
    "    print(f\"AI plays as {'White' if ai_side == chess.WHITE else 'Black'}.\")\n",
    "    print(\"Enter moves in UCI notation (e.g., e2e4). Type 'resign' to concede.\")\n",
    "    \n",
    "    while not state.is_terminal():\n",
    "        print_board(state.board)\n",
    "        if state.board.turn == ai_side:\n",
    "            move = trainer.mcts.run(state, history)\n",
    "            print(f\"AI move: {move.uci()}\")\n",
    "        else:\n",
    "            move = get_valid_user_move(state.board)\n",
    "            if move is None:\n",
    "                print(\"You resigned.\")\n",
    "                log_game_result(log_dir, \"human\", ai_side, \"AI wins by resignation\", moves)\n",
    "                return 'AI wins by resignation'\n",
    "        moves.append(move)\n",
    "        state.apply_move(move)\n",
    "        history.append(state.clone())\n",
    "        history = history[-8:]\n",
    "    \n",
    "    print_board(state.board)\n",
    "    result = state.result()\n",
    "    if result == '1-0':\n",
    "        winner = 'White' if ai_side == chess.BLACK else 'AI'\n",
    "    elif result == '0-1':\n",
    "        winner = 'Black' if ai_side == chess.WHITE else 'AI'\n",
    "    else:\n",
    "        winner = 'Draw'\n",
    "    print(f\"Game over. Result: {result} ({winner})\")\n",
    "    log_game_result(log_dir, \"human\", ai_side, f\"{result} ({winner})\", moves)\n",
    "    return result\n",
    "\n",
    "def play_against_stockfish(trainer, stockfish_path, ai_side, log_dir, stockfish_depth=10):\n",
    "    stockfish = Stockfish(path=stockfish_path)\n",
    "    stockfish.set_depth(stockfish_depth)\n",
    "    board = chess.Board()\n",
    "    state = ChessState(board)\n",
    "    history = [state.clone()]\n",
    "    moves = []\n",
    "    \n",
    "    print(f\"AI plays as {'White' if ai_side == chess.WHITE else 'Black'} against Stockfish.\")\n",
    "    \n",
    "    while not state.is_terminal():\n",
    "        if state.board.turn == ai_side:\n",
    "            move = trainer.mcts.run(state, history)\n",
    "        else:\n",
    "            stockfish.set_fen_position(state.board.fen())\n",
    "            move = chess.Move.from_uci(stockfish.get_best_move())\n",
    "        moves.append(move)\n",
    "        state.apply_move(move)\n",
    "        history.append(state.clone())\n",
    "        history = history[-8:]\n",
    "    \n",
    "    result = state.result()\n",
    "    print(f\"Game over. Result: {result}\")\n",
    "    log_game_result(log_dir, \"stockfish\", ai_side, result, moves)\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = DeepNeuralNetwork(in_channels=103, out_channels=256, num_residual=10, hidden_size=256).to(device)\n",
    "    trainer = AlphaZeroTrainer(model, device)\n",
    "    log_dir = \"logs\"\n",
    "\n",
    "    # Load latest checkpoint if available\n",
    "    checkpoint_files = [f for f in os.listdir(trainer.checkpoint_dir) if f.endswith('.h5')]\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        checkpoint_path = os.path.join(trainer.checkpoint_dir, latest_checkpoint)\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        model.load_from_hdf5(checkpoint_path)\n",
    "        trainer.games_played = int(latest_checkpoint.split('_')[-1].split('.')[0])\n",
    "    else:\n",
    "        print(\"No checkpoints found.\")\n",
    "        # Load training data and train if available\n",
    "        training_files = [f for f in os.listdir(trainer.training_dir) if f.endswith('.h5')]\n",
    "        if training_files:\n",
    "            print(\"Loading training data...\")\n",
    "            trainer.load_training_data()\n",
    "            if len(trainer.replay_buffer) > 0:\n",
    "                print(\"Training model with loaded data...\")\n",
    "                trainer.train(epochs=1, batch_size=128)\n",
    "            trainer.games_played += len(training_files)\n",
    "\n",
    "    # User chooses opponent and AI side\n",
    "    opponent = input(\"Choose opponent (human/stockfish): \").strip().lower()\n",
    "    while opponent not in ['human', 'stockfish']:\n",
    "        print(\"Invalid choice. Please choose 'human' or 'stockfish'.\")\n",
    "        opponent = input(\"Choose opponent (human/stockfish): \").strip().lower()\n",
    "\n",
    "    side = input(\"Choose AI side (white/black/random): \").strip().lower()\n",
    "    while side not in ['white', 'black', 'random']:\n",
    "        print(\"Invalid choice. Please choose 'white', 'black', or 'random'.\")\n",
    "        side = input(\"Choose AI side (white/black/random): \").strip().lower()\n",
    "\n",
    "    ai_side = chess.WHITE if side == 'white' else chess.BLACK if side == 'black' else random.choice([chess.WHITE, chess.BLACK])\n",
    "\n",
    "    # Play the game\n",
    "    if opponent == 'human':\n",
    "        result = play_against_user(trainer, ai_side, log_dir)\n",
    "    else:\n",
    "        stockfish_path = \"stockfish/stockfish/stockfish-windows-x86-64-avx2.exe\"\n",
    "        result = play_against_stockfish(trainer, stockfish_path, ai_side, log_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
